base_config: ./base.yaml
task_cls: tasks.vocoder.melgan.MelGANTask

###########################################################
#         GENERATOR NETWORK ARCHITECTURE SETTING          #
###########################################################
generator_params:
  in_channels: 80               # Number of input channels.
  out_channels: 1               # Number of output channels.
  kernel_size: 7                # Kernel size of initial and final conv layers.
  channels: 512                 # Initial number of channels for conv layers.
  upsample_scales: [ 8, 8, 4 ]    # List of Upsampling scales.
  stack_kernel_size: 3          # Kernel size of dilated conv layers in residual stack.
  stacks: 4                     # Number of stacks in a single residual stack module.
  use_weight_norm: True         # Whether to use weight normalization.
  use_causal_conv: False        # Whether to use causal convolution.
  aux_context_window: 0
  use_nsf: false
  use_pitch_embed: false
###########################################################
#       DISCRIMINATOR NETWORK ARCHITECTURE SETTING        #
###########################################################
discriminator_params:
  in_channels: 1                    # Number of input channels.
  out_channels: 1                   # Number of output channels.
  scales: 3                         # Number of multi-scales.
  downsample_pooling: "AvgPool1d"   # Pooling type for the input downsampling.
  downsample_pooling_params: # Parameters of the above pooling function.
    kernel_size: 4
    stride: 2
    padding: 1
    count_include_pad: False
  kernel_sizes: [ 5, 3 ]              # List of kernel size.
  channels: 16                      # Number of channels of the initial conv layer.
  max_downsample_channels: 512      # Maximum number of channels of downsampling layers.
  downsample_scales: [ 4, 4, 4 ]      # List of downsampling scales.
  nonlinear_activation: "LeakyReLU" # Nonlinear activation function.
  nonlinear_activation_params: # Parameters of nonlinear activation function.
    negative_slope: 0.2
  use_weight_norm: True             # Whether to use weight norm.

###########################################################
#                   STFT LOSS SETTING                     #
###########################################################
stft_loss_params:
  fft_sizes: [ 1024, 2048, 512 ]  # List of FFT size for STFT-based loss.
  hop_sizes: [ 120, 240, 50 ]     # List of hop size for STFT-based loss
  win_lengths: [ 600, 1200, 240 ] # List of window length for STFT-based loss.
  window: "hann_window"         # Window function for STFT-based loss
use_mel_loss: true

###########################################################
#               ADVERSARIAL LOSS SETTING                  #
###########################################################
use_feat_match_loss: false # Whether to use feature matching loss.
lambda_adv: 2.5            # Loss balancing coefficient for adversarial loss.

###########################################################
#             OPTIMIZER & SCHEDULER SETTING               #
###########################################################
generator_optimizer_params:
  lr: 1.0e-3                          # Generator's learning rate.
  eps: 1.0e-7                         # Generator's epsilon.
  weight_decay: 0.0                   # Generator's weight decay coefficient.
  amsgrad: true
generator_grad_norm: -1                 # Generator's gradient norm.
generator_scheduler_type: "MultiStepLR" # Generator's scheduler type.
generator_scheduler_params:
  gamma: 0.5                          # Generator's scheduler gamma.
  milestones: # At each milestone, lr will be multiplied by gamma.
    - 100000
    - 200000
    - 300000
    - 400000
    - 500000
    - 600000
discriminator_optimizer_params:
  lr: 1.0e-3                              # Discriminator's learning rate.
  eps: 1.0e-7                             # Discriminator's epsilon.
  weight_decay: 0.0                       # Discriminator's weight decay coefficient.
  amsgrad: true
discriminator_grad_norm: -1                 # Discriminator's gradient norm.
discriminator_scheduler_type: "MultiStepLR" # Discriminator's scheduler type.
discriminator_scheduler_params:
  gamma: 0.5                              # Discriminator's scheduler gamma.
  milestones: # At each milestone, lr will be multiplied by gamma.
    - 100000
    - 200000
    - 300000
    - 400000
    - 500000
    - 600000

# training
max_samples: 16384
max_sentences: 48
disc_start_steps: 200000 # Number of steps to start to train discriminator.
max_updates: 2000000
